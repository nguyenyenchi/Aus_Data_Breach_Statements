{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32b16493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "709dc271",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a64c76-27da-4254-ace5-6409be5dc9b8",
   "metadata": {},
   "source": [
    "### Plot year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd26a2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to display values on barcharts\n",
    "def show_values(axs, orient=\"v\", space=.01):\n",
    "    def _single(ax):\n",
    "        if orient == \"v\":\n",
    "            for p in ax.patches:\n",
    "                _x = p.get_x() + p.get_width() / 2\n",
    "                _y = p.get_y() + p.get_height() + (p.get_height()*0.01)\n",
    "                value = '{:.0f}'.format(p.get_height(), fontsize=14)\n",
    "                ax.text(_x, _y, value, ha=\"center\", fontsize=12) \n",
    "        elif orient == \"h\":\n",
    "            for p in ax.patches:\n",
    "                _x = p.get_x() + p.get_width() + float(space)\n",
    "                _y = p.get_y() + p.get_height() - (p.get_height()*0.5)\n",
    "                value = '{:.0f}'.format(p.get_width(), fontsize=14)\n",
    "                ax.text(_x, _y, value, ha=\"left\", fontsize=12)\n",
    "\n",
    "    if isinstance(axs, np.ndarray):\n",
    "        for idx, ax in np.ndenumerate(axs):\n",
    "            _single(ax)\n",
    "    else:\n",
    "        _single(axs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a165d1",
   "metadata": {},
   "source": [
    "## II. Feature Engineering\n",
    "\n",
    "- Word count \n",
    "\n",
    "- Character count \n",
    "\n",
    "- Sentence count \n",
    "\n",
    "- Key word / unique word count \n",
    "\n",
    "- Cybersecurity word count \n",
    "\n",
    "- Numerical terms \n",
    "\n",
    "- Average word length \n",
    "\n",
    "- Average sentence length \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b0b9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of tokens ie. sequence of characters\n",
    "len(df['Statement'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b2e762",
   "metadata": {},
   "source": [
    "### 1. Word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20328a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ffd538",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['WordCount'] = df['Statement'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6b65f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average word length\n",
    "df['WordCount'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca96f9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Statement'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c11b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character count???\n",
    "def character_count(text):\n",
    "    character_count = 0\n",
    "    word_list = text.split()\n",
    "    for word in word_list:\n",
    "        character_count += len(word)\n",
    "    return character_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac6f06e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e73d25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e476cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "saying = ['After', 'all', 'is', 'said', 'and', 'done',\n",
    "        'more', 'is', 'said', 'than', 'done']\n",
    "tokens = set(saying)\n",
    "print(tokens)\n",
    "\n",
    "tokens = sorted(tokens)\n",
    "tokens[-2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b40099",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac8230b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def preprocess(text):\n",
    "    # split into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # split into words\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    \n",
    "    # remove punctuation from each word\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    \n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    \n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    \n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    # Join the tokens back into a string\n",
    "    processed_text = ' '.join(lemmatized_tokens)\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e00a20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CleanStatement'] = df['Statement'].apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229158bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdc3f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c255086",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['NotificationDate'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7f54c4",
   "metadata": {},
   "source": [
    "### Get sentiment using SiEBERT - English-Language Sentiment Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688619b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref: https://huggingface.co/siebert/sentiment-roberta-large-english\n",
    "\n",
    "from transformers import pipeline\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\", model=\"siebert/sentiment-roberta-large-english\")\n",
    "# print(sentiment_analysis(\"I love this!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251f97cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to get sentiment\n",
    "def get_sentiment(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    sent_dict = {'Positive': 0, 'Negative': 0}\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Get sentiment for each sentence in the statement\n",
    "        sentiment = sentiment_analysis(sentence)   \n",
    "        if sentiment[0]['label'] == 'NEGATIVE':\n",
    "            sent_dict['Negative'] += 1\n",
    "        else: \n",
    "            sent_dict['Positive'] += 1\n",
    "    # Check Positive or Negative has higher values\n",
    "    if sent_dict['Positive'] > sent_dict['Negative']:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641287d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Siebert_Sentiment'] = df['Statement'].apply(lambda x: get_sentiment(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77227294",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Siebert_Sentiment'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a600a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c485ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = df['Siebert_Sentiment'].value_counts()\n",
    "\n",
    "# Create bar plot\n",
    "plt.figure(figsize=(6, 4))\n",
    "p = sns.barplot(x=value_counts.index, y=value_counts.values, color='#035397')\n",
    "show_values(p)\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Sentiment Analysis of Statements')\n",
    "plt.xticks(fontsize=8)\n",
    "plt.yticks(fontsize=8)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f46ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Length'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44817164",
   "metadata": {},
   "source": [
    "### Import databreaches Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9653cde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataframe with Description split into 2 columns: DescriptionClean (text only - need more cleaning) & WebsiteLink\n",
    "\n",
    "columns_to_read = ['Name', 'Day', 'Month', 'Year', 'NotificationDate', 'WebpageTitle', 'Author',\n",
    "                   'DetailedExplanation', 'Whitewashing', 'Apology', 'Compensation',\n",
    "                  'ResponsiveAction', 'ValueCommitment', 'CustomerFocus', 'OpenDisclosure', 'CustomerAdvice']\n",
    "df_st = pd.read_csv('ausdatabreach2018-23v1.csv', usecols=columns_to_read, encoding='latin1')\n",
    "df_st.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570879e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_st.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa7b798",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_st.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9943d6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Name'].equals(df_st['Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b49a206",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = df['Name'] != df_st['Name']\n",
    "\n",
    "# Get the indices of the differing rows\n",
    "differing_rows = comparison[comparison].index\n",
    "\n",
    "# Print the differing rows\n",
    "print(differing_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c792e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_st['Name'].iloc[39:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778fc092",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Name'].iloc[39:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b902c9",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d63a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63879274",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ed07e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d3958b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e214c784",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Calculate value counts\n",
    "value_counts = df['Year'].value_counts()\n",
    "\n",
    "# Create bar plot\n",
    "p = sns.barplot(x=value_counts.index, y=value_counts.values, color='#035397')\n",
    "show_values(p)\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Number of breaches by year')\n",
    "plt.xticks(fontsize=8)\n",
    "plt.yticks(fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0984fec5",
   "metadata": {},
   "source": [
    "### Percentages of each characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29011bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DetailedExplanation'].value_counts(normalize=True)[1]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04baedb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up the figure and axes\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "boolean_col = ['DetailedExplanation', 'Whitewashing', 'Apology', \n",
    "               'Compensation', 'ResponsiveAction', 'ValueCommitment',\n",
    "                'CustomerFocus', 'OpenDisclosure', 'CustomerAdvice']\n",
    "sns.countplot(data=df, x=\"DetailedExplanation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f7e468",
   "metadata": {},
   "outputs": [],
   "source": [
    "boolean_col = ['DetailedExplanation', 'Whitewashing', 'Apology', \n",
    "               'Compensation', 'ResponsiveAction', 'ValueCommitment',\n",
    "                'CustomerFocus', 'OpenDisclosure', 'CustomerAdvice']\n",
    "\n",
    "values = []\n",
    "for col in boolean_col:\n",
    "    pct = df[col].value_counts(normalize=True)[1]*100\n",
    "    values.append(pct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c23742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bar plot\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "p = sns.barplot(x=boolean_col, y=values, color='#035397')\n",
    "show_values(p)\n",
    "plt.xlabel('Characteristics')\n",
    "plt.ylabel('Percentages')\n",
    "plt.title('Percentages of Characteristics')\n",
    "plt.xticks(fontsize=7)\n",
    "plt.yticks(fontsize=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2b5e1c",
   "metadata": {},
   "source": [
    "### Response Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a63c282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FullTransparency\n",
    "def type1(row):\n",
    "    \n",
    "    if (row['DetailedExplanation'] == 1) & (row['Whitewashing'] == 0) & (row['ResponsiveAction'] == 1) & (row['ValueCommitment'] == 1)  & (row['OpenDisclosure'] == 1):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Apply the custom function to create a new column\n",
    "df['FullTransparency'] = df.apply(type1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7899fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['FullTransparency']==1].iloc[:,7:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b20b68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guarded\n",
    "def type2(row):\n",
    "    \n",
    "    if (row['DetailedExplanation'] == 0) & (row['Whitewashing'] == 1) & (row['ResponsiveAction'] == 1) & (row['ValueCommitment'] == 1)  & (row['OpenDisclosure'] == 0):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Apply the custom function to create a new column\n",
    "df['Guarded'] = df.apply(type2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf21472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opacity\n",
    "def type3(row):\n",
    "    \n",
    "    if (row['DetailedExplanation'] == 0) & (row['Whitewashing'] == 1) & (row['ResponsiveAction'] == 1) & (row['ValueCommitment'] == 0)  & (row['OpenDisclosure'] == 0):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Apply the custom function to create a new column\n",
    "df['Opacity'] = df.apply(type3, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d95c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CustomerInterest\n",
    "def type4(row):\n",
    "    \n",
    "    if (row['Apology'] == 1) & (row['Compensation'] == 1) & (row['CustomerFocus'] == 1) & (row['CustomerAdvice'] == 1):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Apply the custom function to create a new column\n",
    "df['CustomerInterest'] = df.apply(type4, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdcffe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CustomerInterest\n",
    "def type5(row):\n",
    "    \n",
    "    if (row['Apology'] == 1) & (row['Compensation'] == 0):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Apply the custom function to create a new column\n",
    "df['BalancedInterest'] = df.apply(type5, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb7e66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CustomerInterest\n",
    "def type6(row):\n",
    "    \n",
    "    if (row['Apology'] == 0) & (row['Compensation'] == 0) & (row['CustomerFocus'] == 0) & (row['CustomerAdvice'] == 0):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Apply the custom function to create a new column\n",
    "df['CompanyInterest'] = df.apply(type6, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614b0d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:5,7:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20003b6a",
   "metadata": {},
   "source": [
    "### Percentages of each response types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62073ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_col = ['FullTransparency', 'Guarded', 'Opacity', \n",
    "               'CustomerInterest', 'BalancedInterest', 'CompanyInterest']\n",
    "\n",
    "values_pct = []\n",
    "for col in response_col:\n",
    "    # If there is only one value \n",
    "    if len(df[col].value_counts(normalize=True)) == 1:\n",
    "        values_pct.append(0)\n",
    "        \n",
    "    else:\n",
    "        pct = df[col].value_counts(normalize=True)[1]*100\n",
    "        \n",
    "        values_pct.append(pct)\n",
    "    \n",
    "# Create bar plot\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "p = sns.barplot(x=response_col, y=values_pct, color='#035397')\n",
    "show_values(p)\n",
    "plt.xlabel('Response Types')\n",
    "plt.ylabel('Percentages')\n",
    "plt.title('Percentages of Response Types')\n",
    "plt.xticks(fontsize=5)\n",
    "plt.yticks(fontsize=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49810c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BalancedInterest'].value_counts(normalize=True)[1]*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b19a3db",
   "metadata": {},
   "source": [
    "### Bigrams for titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8ddbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245a9208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref: https://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n",
    "\n",
    "# Lemmatize with POS Tag\n",
    "from nltk.corpus import wordnet\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f875f27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    try: \n",
    "        # split into sentences\n",
    "        sentences = sent_tokenize(text)\n",
    "\n",
    "        # split into words\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        # convert to lower case\n",
    "        tokens = [w.lower() for w in tokens]\n",
    "\n",
    "        # remove punctuation from each word\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        stripped = [w.translate(table) for w in tokens]\n",
    "\n",
    "        # remove remaining tokens that are not alphabetic\n",
    "        words = [word for word in stripped if word.isalpha()]\n",
    "\n",
    "        # filter out stop words\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        words = [w for w in words if not w in stop_words]\n",
    "\n",
    "        # Lemmatise the tokens\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in words]\n",
    "\n",
    "        # Join the tokens back into a string\n",
    "        processed_text = ' '.join(lemmatized_tokens)\n",
    "        return processed_text\n",
    "    except TypeError:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e541eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Title_nltk'] = df['WebpageTitle'].apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03cf637",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Title_nltk'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7f26de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "def bigrams_convert(column, n=2):\n",
    "    df['bigrams'+'_'+column]=df[column].apply(lambda sentence: list(ngrams(sentence.split(), n)))\n",
    "    \n",
    "def trigrams_convert(column, n=3):\n",
    "    df['trigrams'+'_'+column]=df[column].apply(lambda sentence: list(ngrams(sentence.split(), n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0dd416",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_convert('Title_nltk')\n",
    "trigrams_convert('Title_nltk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde8b6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcb0942",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "bigrams = []\n",
    "for row in df['bigrams_Title_nltk']:\n",
    "    bigrams.extend(row)\n",
    "    \n",
    "print(f'Count of bigrams: {len(bigrams)}')\n",
    "print(\"\\n\")\n",
    "    \n",
    "# Count the frequency of each bigram\n",
    "bigram_frequency = Counter(bigrams)\n",
    "\n",
    "# Print the frequency of each bigram\n",
    "# for bigram, frequency in bigram_frequency.items():\n",
    "#     print(bigram, frequency)\n",
    "    \n",
    "# Organize elements by frequency using most_common()\n",
    "bigrams_organized_by_frequency = bigram_frequency.most_common()\n",
    "\n",
    "# Print the elements organized by frequency\n",
    "for element, frequency in bigrams_organized_by_frequency:\n",
    "    print(element, frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c48cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "bigrams_organized_by_frequency_10 = bigram_frequency.most_common(5)\n",
    "\n",
    "# Print the elements organized by frequency\n",
    "# print('Top 10 trigrams')\n",
    "bi = []\n",
    "freq = []\n",
    "\n",
    "for element, frequency in bigrams_organized_by_frequency_10:\n",
    "    bigram = ' '.join(element)\n",
    "    print(f'Bigram: {bigram} - Frequency: {frequency}')\n",
    "    bi.append(bigram)\n",
    "    freq.append(frequency)\n",
    "\n",
    "\n",
    "df_bi = pd.DataFrame({'Bigram': bi, 'Count': freq})\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "ax = sns.barplot(x=df_bi['Bigram'].values, y=df_bi['Count'].values)\n",
    "\n",
    "# Display the count on each bar\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_height(), '.0f'), (p.get_x() + p.get_width() / 2., p.get_height()), ha = 'center', va = 'center', xytext = (0, 5), textcoords = 'offset points')\n",
    "\n",
    "plt.ylabel('Count')\n",
    "plt.title('Top 5 Most Frequently Occuring Bigrams')\n",
    "\n",
    "plt.xticks(rotation=70)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4215844",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = []\n",
    "for row in df['trigrams_Title_nltk']:\n",
    "    trigrams.extend(row)\n",
    "    \n",
    "print(f'Count of trigrams: {len(trigrams)}')\n",
    "print(\"\\n\")\n",
    "    \n",
    "# Count the frequency of each trigram\n",
    "trigram_frequency = Counter(trigrams)\n",
    "\n",
    "# Organize elements by frequency using most_common()\n",
    "trigrams_organized_by_frequency = trigram_frequency.most_common()\n",
    "\n",
    "# Print the elements organized by frequency\n",
    "for element, frequency in trigrams_organized_by_frequency:\n",
    "    print(element, frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4fc838",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams_organized_by_frequency_10 = trigram_frequency.most_common(5)\n",
    "\n",
    "# Print the elements organized by frequency\n",
    "# print('Top 10 trigrams')\n",
    "tri = []\n",
    "freq = []\n",
    "\n",
    "for element, frequency in trigrams_organized_by_frequency_10:\n",
    "    trigram = ' '.join(element)\n",
    "#     print(f'Trigram: {trigram} - Frequency: {frequency}')\n",
    "    tri.append(trigram)\n",
    "    freq.append(frequency)\n",
    "    \n",
    "df_tri = pd.DataFrame({'Trigram': tri, 'Count': freq})\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "ax = sns.barplot(x=df_tri['Trigram'].values, y=df_tri['Count'].values)\n",
    "\n",
    "# Display the count on each bar\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_height(), '.0f'), (p.get_x() + p.get_width() / 2., p.get_height()), ha = 'center', va = 'center', xytext = (0, 5), textcoords = 'offset points')\n",
    "\n",
    "plt.ylabel('Count')\n",
    "plt.title('Top 5 Most Frequently Occuring Trigrams')\n",
    "\n",
    "plt.xticks(rotation=70)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9225eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate value counts\n",
    "value_counts = df['Author'].value_counts(dropna=False)\n",
    "\n",
    "# Create bar plot\n",
    "plt.figure(figsize=(9, 5))\n",
    "p = sns.barplot(x=value_counts.index, y=value_counts.values, color='#035397')\n",
    "show_values(p)\n",
    "plt.xlabel('Author')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Authors of Statements')\n",
    "plt.xticks(fontsize=8)\n",
    "plt.yticks(fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b3e4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Author'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c000d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_excel('output.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c53ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.drop(['CleanStatement2'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68961af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize NLTK sentiment analyzer\n",
    "# nltk.download('vader_lexicon')\n",
    "# from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "# analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# # create get_sentiment function\n",
    "# def get_sentiment(text):\n",
    "#     scores = analyzer.polarity_scores(text)\n",
    "#     sentiment = 1 if scores['pos'] > 0 else 0\n",
    "#     return sentiment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
