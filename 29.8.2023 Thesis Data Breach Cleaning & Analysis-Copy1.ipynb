{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32b16493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "709dc271",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd9a79e",
   "metadata": {},
   "source": [
    "#### Get all files in the Statements folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "656cdca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2018.01.31 GoGet.txt', '2018.03.22 Facebook.txt', '2018.03.29 Under Armour.txt', '2018.05.06 PageUp People.txt', '2018.05.14 Family Planning NSW.txt', '2018.06.04 MyHeritage.txt', '2018.06.04 Southport Sharks.txt', '2018.06.14 Motorcycle Accessories Supermarket Pty Ltd.txt', '2018.06.22 PEXA.txt', '2018.07.04 RCR Tomlinson.txt', '2018.09.08 The Perth Mint.txt', '2018.09.28 Facebook.txt', '2018.11.01 Austal.txt', '2018.11.30 Marriott.txt', '2018.12.27 Nova.txt', '2019.01.05 Early Warning Network.txt', '2019.02.05 Landmark White.txt', '2019.02.14 Coffee Meets Bagel.txt', '2019.03.13 Kathmandu.txt', '2019.05.24 Canva.txt', '2019.06.04 Australian National University.txt', '2019.06.17 Australian Catholic University.txt', '2019.07.26 NAB.txt', '2019.07.29 Sephora.txt', '2019.10.17 Optus.txt', '2019.11.12 Monash IVF.txt', '2019.12.02 Vistaprint.txt', '2019.12.10 Australian Sports Commission.txt', '2020.01.16 P&N Bank.txt', '2020.03.11 Melbourne Polytechnic.txt', '2020.03.23 Henning Harders.txt', '2020.03.31 Federal Court of Australia.txt', '2020.05.05 Toll Group.txt', '2020.05.28 Service NSW.txt', '2020.05.29 Big Interest Group (BigFooty).com.txt', '2020.06.09 Lion.txt', '2020.07.20 WA Department of Health.txt', '2020.07.30 Appen.txt', '2020.08.03 Regis Healthcare.txt', '2020.08.07 ProctorU.txt', '2020.08.14 ACT Education Directorate.txt', '2020.08.16 iSignthis.txt', '2020.09.02 Anglicare Sydney.txt', '2020.09.21 University of Tasmania.txt', '2020.09.24 Scouts Victoria.txt', '2020.09.30 Department of Foreign Affairs and Trade.txt', '2020.10.21 Nitro Software.txt', '2020.10.28 City of Port Phillip council.txt', '2020.11.23 Law In Order.txt', '2020.11.30 BTC Markets.txt', '2021.02.04 Oxfam Australia.txt', '2021.02.05 Ambulance Tasmania.txt', '2021.04.16 Swinburne University of Technology.txt', '2021.05.24 TPG Telecom.txt', '2021.07.08 NSW Department of Education.txt', '2021.11.10 SA Ambulance Service.txt', '2021.11.16 Frontier Software.txt', '2022.03.22 Okta.txt', '2022.04.19 Department of Home Affairs.txt', '2022.05.04 National Tertiary Education Union.txt', '2022.05.04 Transport for NSW.txt', '2022.05.27 Spirit Super.txt', '2022.05.30 CTARS.txt', '2022.07.12 Deakin University.txt', '2022.07.29 University of Western Australia.txt', '2022.09.04 Fremantle Football Club.txt', '2022.09.20 Optus.txt', '2022.10.05 icare.txt', '2022.10.08 Costa Group.txt', '2022.10.11 Dialog.txt', '2022.10.13 Medibank.txt', '2022.10.14 MyDeal (Woolworths Group).txt', '2022.10.17 Vinomofo Australia.txt', '2022.10.27 Medlab Pathology (Australian Clinical Labs).txt', '2022.10.27 SSKB.txt', '2022.11.03 Harcourts.txt', '2022.11.04 BWX (Flora & Fauna).txt', '2022.11.04 Legal Aid ACT.txt', '2022.11.06 PNORS Technology Group.txt', '2022.11.22 The Smith Family.txt', '2022.11.23 Xavier College.txt', '2022.12.09 Telstra.txt', '2022.12.14 TPG Telecom.txt', '2022.12.23 Queensland University of Technology.txt', '2023.01.06 Fire Rescue Victoria.txt', '2023.01.29 Mount Lilydale Mercy College.txt', '2023.02.23 My Rewards International.txt', '2023.03.16 iph.txt', '2023.03.20 Latitude.txt', '2023.03.29 Meriton Suites.txt', '2023.04.03 Service NSW.txt', '2023.04.04 TAFE South Australia.txt', '2023.04.28 Amnesty International Australia.txt', '2023.05.10 Dr Felix Chan.txt', '2023.05.19 Ambulance Victoria.txt', '2023.05.26 NT Health.txt', '2023.05.29 PwC.txt', '2023.05.31 Toyota.txt', '2023.06.16 Smartpay.txt', '2023.06.21 HWL Ebsworth.txt', '2023.08.04 Aristocrat.txt']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "folder_path = \"Statements\"  # Replace with the path to your folder\n",
    "\n",
    "# Get all file paths in the folder\n",
    "file_paths = glob.glob(folder_path + \"/*\")\n",
    "\n",
    "all_files = []\n",
    "# Iterate over the file paths and print their names\n",
    "for file_path in file_paths:\n",
    "    file_name = os.path.basename(file_path)\n",
    "    all_files.append(file_name)\n",
    "print(all_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9ea735",
   "metadata": {},
   "source": [
    "#### Number of statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c21fba42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffea4bf",
   "metadata": {},
   "source": [
    "#### Read each file / statement into a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd57ff34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_as_string(file_path):\n",
    "    with open('Statements/'+file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "        text = ' '.join(lines).replace(\"\\n\", \"\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a63f1195",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "statements = []\n",
    "companies = []\n",
    "dates = []\n",
    "years = []\n",
    "\n",
    "# Function to format date\n",
    "def format_date(date):\n",
    "    # Get date\n",
    "    date_str = date\n",
    "\n",
    "    # Convert the date string to a datetime object\n",
    "    try:\n",
    "        date_obj = datetime.strptime(date_str, \"%Y.%m.%d\")\n",
    "        # Convert the datetime object to the desired format\n",
    "        formatted_date = date_obj.strftime(\"%d/%m/%Y\")\n",
    "        return formatted_date\n",
    "    except ValueError:\n",
    "        return None  \n",
    "\n",
    "for file_name in all_files:\n",
    "    # Get statement\n",
    "    statements.append(read_file_as_string(file_name))\n",
    "    # Get company name from file name\n",
    "    companies.append(file_name[11:-4])\n",
    "    \n",
    "    # Transform date into correct datetime format\n",
    "    date = file_name[:10]\n",
    "    formatted_date = format_date(date)\n",
    "    dates.append(formatted_date)\n",
    "    \n",
    "    # Get year\n",
    "    year = int(file_name[:4])\n",
    "    years.append(year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf83491a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 101 entries, 0 to 100\n",
      "Data columns (total 4 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   Name              101 non-null    object\n",
      " 1   NotificationDate  101 non-null    object\n",
      " 2   Year              101 non-null    int64 \n",
      " 3   Statement         101 non-null    object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 3.3+ KB\n"
     ]
    }
   ],
   "source": [
    "data = {'Name': companies, 'NotificationDate': dates, 'Year': years, 'Statement': statements}\n",
    "df = pd.DataFrame(data=data)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0446c5f-922e-46ee-b2e0-a0ca39deece2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>NotificationDate</th>\n",
       "      <th>Year</th>\n",
       "      <th>Statement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GoGet</td>\n",
       "      <td>31/01/2018</td>\n",
       "      <td>2018</td>\n",
       "      <td>A statement from GoGetâ€™s CEO To our valued mem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Facebook</td>\n",
       "      <td>22/03/2018</td>\n",
       "      <td>2018</td>\n",
       "      <td>I want to share an update on the Cambridge Ana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Under Armour</td>\n",
       "      <td>29/03/2018</td>\n",
       "      <td>2018</td>\n",
       "      <td>To the MyFitnessPal Community:  We are writing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PageUp People</td>\n",
       "      <td>06/05/2018</td>\n",
       "      <td>2018</td>\n",
       "      <td>Information update 5 June 2018  As part of our...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Family Planning NSW</td>\n",
       "      <td>14/05/2018</td>\n",
       "      <td>2018</td>\n",
       "      <td>We are writing to personally inform you and ap...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Name NotificationDate  Year  \\\n",
       "0                GoGet       31/01/2018  2018   \n",
       "1             Facebook       22/03/2018  2018   \n",
       "2         Under Armour       29/03/2018  2018   \n",
       "3        PageUp People       06/05/2018  2018   \n",
       "4  Family Planning NSW       14/05/2018  2018   \n",
       "\n",
       "                                           Statement  \n",
       "0  A statement from GoGetâ€™s CEO To our valued mem...  \n",
       "1  I want to share an update on the Cambridge Ana...  \n",
       "2  To the MyFitnessPal Community:  We are writing...  \n",
       "3  Information update 5 June 2018  As part of our...  \n",
       "4  We are writing to personally inform you and ap...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f92998",
   "metadata": {},
   "source": [
    "#### Determine the number of rows and columns in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "167d40c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e3c5ba",
   "metadata": {},
   "source": [
    "#### Find number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f0f8e96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba337734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2022    27\n",
       "2020    22\n",
       "2023    17\n",
       "2018    15\n",
       "2019    13\n",
       "2021     7\n",
       "Name: Year, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Year'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a64c76-27da-4254-ace5-6409be5dc9b8",
   "metadata": {},
   "source": [
    "### Plot year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd26a2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to display values on barcharts\n",
    "def show_values(axs, orient=\"v\", space=.01):\n",
    "    def _single(ax):\n",
    "        if orient == \"v\":\n",
    "            for p in ax.patches:\n",
    "                _x = p.get_x() + p.get_width() / 2\n",
    "                _y = p.get_y() + p.get_height() + (p.get_height()*0.01)\n",
    "                value = '{:.0f}'.format(p.get_height(), fontsize=14)\n",
    "                ax.text(_x, _y, value, ha=\"center\", fontsize=12) \n",
    "        elif orient == \"h\":\n",
    "            for p in ax.patches:\n",
    "                _x = p.get_x() + p.get_width() + float(space)\n",
    "                _y = p.get_y() + p.get_height() - (p.get_height()*0.5)\n",
    "                value = '{:.0f}'.format(p.get_width(), fontsize=14)\n",
    "                ax.text(_x, _y, value, ha=\"left\", fontsize=12)\n",
    "\n",
    "    if isinstance(axs, np.ndarray):\n",
    "        for idx, ax in np.ndenumerate(axs):\n",
    "            _single(ax)\n",
    "    else:\n",
    "        _single(axs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebc1d415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "value_counts = df['Year'].value_counts()\n",
    "\n",
    "# Create bar plot\n",
    "plt.figure(figsize=(7, 5))\n",
    "p = sns.barplot(x=value_counts.index, y=value_counts.values, color='#035397')\n",
    "show_values(p)\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Number of Statements by Year')\n",
    "plt.xticks(fontsize=8)\n",
    "plt.yticks(fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f157c745-c467-4a81-80ba-75dfd8df033c",
   "metadata": {},
   "source": [
    "### Ref: https://github.com/j-hartmann/siebert/blob/main/SieBERT.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54b4c231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.27.4\n"
     ]
    }
   ],
   "source": [
    "# Check version of transformers\n",
    "import transformers\n",
    "print(transformers.__version__)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9a4a4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dependencies\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# specify path of pretrained model\n",
    "checkpoint = \"siebert/sentiment-roberta-large-english\"  # SieBERT\n",
    "\n",
    "# load pretrained tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "51320d8e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_sentiment(text):\n",
    "    sequences = sent_tokenize(text)\n",
    "\n",
    "    # tokenize sequences\n",
    "    tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    # predict with model\n",
    "    output = model(**tokens)\n",
    "\n",
    "    # transform logits to class labels\n",
    "    predictions = torch.nn.functional.softmax(output.logits, dim=-1)\n",
    "\n",
    "    confidences = predictions.max(1)[0].tolist()\n",
    "\n",
    "    classes = predictions.argmax(-1).tolist()\n",
    "\n",
    "    # labels = pd.Series(classes).map(model.config.id2label)\n",
    "    \n",
    "    # consolidate results into a dataframe\n",
    "    sentiment = pd.DataFrame(list(zip(classes, confidences)), columns=['class', 'confidence'])\n",
    "    \n",
    "    # Get counts\n",
    "    sent_count = sentiment['class'].value_counts()\n",
    "    \n",
    "    # Count of positive sentences\n",
    "    pos_cnt = sent_count[1]\n",
    "\n",
    "    # Count of negative sentences\n",
    "    neg_cnt = sent_count[0]\n",
    "    \n",
    "    # Get averages of confidence scores\n",
    "    confi_avg = sentiment.groupby('class')['confidence'].mean()\n",
    "    \n",
    "    # Average of positive\n",
    "    pos_avg = confi_avg[1]\n",
    "    \n",
    "    # Average of negative\n",
    "    neg_avg = confi_avg[0]\n",
    "    \n",
    "    return pos_cnt, neg_cnt, pos_avg, neg_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "651e9677",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['SentimentScores'] = df['Statement'].apply(lambda x: get_sentiment(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "086b0dbd-942b-4c79-8ae3-00c2959ef782",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_csv('sentiment', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e86059-e641-43ab-ae41-e343a6d2c8cb",
   "metadata": {},
   "source": [
    "### Stop here for sentiment analysis.\n",
    "\n",
    "### Create new notebook for sentiment analysis\n",
    "\n",
    "### Duplicate this notebook for other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44ac3c27-07c1-4fda-8589-eb9cefad8b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9948539018630982\n"
     ]
    }
   ],
   "source": [
    "# Calculate averages of probabilities\n",
    "import statistics as stats\n",
    "\n",
    "print(stats.mean(confidences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "155d46ba-37b3-4d85-a1ba-7f710d4cf5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate total positive sentences ie. Positive 1, Negative 0\n",
    "print(sum(classes))\n",
    "\n",
    "# Calculate total negative sentences\n",
    "len(classes) - sum(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76db7238-ad3d-480d-b027-e2cd7db5d81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count paragraphs in a statement\n",
    "def count_paragraphs(text):\n",
    "    paragraphs = text.split('  ')\n",
    "    return paragraphs\n",
    "\n",
    "d = count_paragraphs(df['Statement'][1])\n",
    "print(d)\n",
    "print(len(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5c9786-5f8d-43e0-a9ef-b87ccfbeda3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Here's a timeline of the events:\n",
    "\"\"\"\n",
    "for index, value in enumerate(d):\n",
    "    if text.strip() in d[index]:\n",
    "        print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05d990f-849f-4397-878c-2b739b77838d",
   "metadata": {},
   "outputs": [],
   "source": [
    "d[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebf8afc-f541-438b-89ab-185f8c4d9728",
   "metadata": {},
   "outputs": [],
   "source": [
    "if text.strip() in d:\n",
    "    print(\"true\")\n",
    "else:\n",
    "    print(\"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c0f0a5-336a-4412-998d-204d507c2f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73e9925-5940-4345-913f-fa0cbe13fc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check which paragraph the sentence belongs to\n",
    "\n",
    "# statement is a list of paragraphs\n",
    "def check_paragraph(text, list_of_paragraphs):\n",
    "    for index, value in enumerate(list_of_paragraphs):\n",
    "        if text.strip() in list_of_paragraphs[index]:\n",
    "            return index\n",
    "\n",
    "\n",
    "sentiment['paragraph'] = sentiment['text'].apply(lambda x: check_paragraph(x, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaa92b3-c908-42af-b42d-a09540ab27e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df0cbbb-8cf3-487a-9a44-66b81e1cf67a",
   "metadata": {},
   "source": [
    "### Simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6347ae8e-c290-4386-92fe-eb7700fbc884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref: https://huggingface.co/siebert/sentiment-roberta-large-english\n",
    "\n",
    "from transformers import pipeline\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\", model=\"siebert/sentiment-roberta-large-english\")\n",
    "# print(sentiment_analysis(\"I love this!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853a6824-0c4e-4b45-9d23-79dc5d6cce9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to get sentiment\n",
    "def get_sentiment(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    sent_dict = {'Positive': 0, 'Negative': 0}\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Get sentiment for each sentence in the statement\n",
    "        sentiment = sentiment_analysis(sentence)   \n",
    "        if sentiment[0]['label'] == 'NEGATIVE':\n",
    "            sent_dict['Negative'] += 1\n",
    "        else: \n",
    "            sent_dict['Positive'] += 1\n",
    "    print(sent_dict)\n",
    "    # Check Positive or Negative has higher values\n",
    "    if sent_dict['Positive'] > sent_dict['Negative']:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0202bc0-2dcd-446d-a0ce-b5d545847d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(df['Statement'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4287be-b6a1-4261-808f-815d7712c004",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1a165d1",
   "metadata": {},
   "source": [
    "## II. Feature Engineering\n",
    "\n",
    "- Word count \n",
    "\n",
    "- Character count \n",
    "\n",
    "- Sentence count \n",
    "\n",
    "- Key word / unique word count \n",
    "\n",
    "- Cybersecurity word count \n",
    "\n",
    "- Numerical terms \n",
    "\n",
    "- Average word length \n",
    "\n",
    "- Average sentence length \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b0b9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of tokens ie. sequence of characters\n",
    "len(df['Statement'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b2e762",
   "metadata": {},
   "source": [
    "### 1. Word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20328a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ffd538",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['WordCount'] = df['Statement'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6b65f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average word length\n",
    "df['WordCount'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca96f9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Statement'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c11b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character count???\n",
    "def character_count(text):\n",
    "    character_count = 0\n",
    "    word_list = text.split()\n",
    "    for word in word_list:\n",
    "        character_count += len(word)\n",
    "    return character_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac6f06e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e73d25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e476cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "saying = ['After', 'all', 'is', 'said', 'and', 'done',\n",
    "        'more', 'is', 'said', 'than', 'done']\n",
    "tokens = set(saying)\n",
    "print(tokens)\n",
    "\n",
    "tokens = sorted(tokens)\n",
    "tokens[-2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b40099",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac8230b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def preprocess(text):\n",
    "    # split into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # split into words\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    \n",
    "    # remove punctuation from each word\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    \n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    \n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    \n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    # Join the tokens back into a string\n",
    "    processed_text = ' '.join(lemmatized_tokens)\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e00a20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CleanStatement'] = df['Statement'].apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229158bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdc3f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c255086",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['NotificationDate'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7f54c4",
   "metadata": {},
   "source": [
    "### Get sentiment using SiEBERT - English-Language Sentiment Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688619b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref: https://huggingface.co/siebert/sentiment-roberta-large-english\n",
    "\n",
    "from transformers import pipeline\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\", model=\"siebert/sentiment-roberta-large-english\")\n",
    "# print(sentiment_analysis(\"I love this!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251f97cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to get sentiment\n",
    "def get_sentiment(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    sent_dict = {'Positive': 0, 'Negative': 0}\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Get sentiment for each sentence in the statement\n",
    "        sentiment = sentiment_analysis(sentence)   \n",
    "        if sentiment[0]['label'] == 'NEGATIVE':\n",
    "            sent_dict['Negative'] += 1\n",
    "        else: \n",
    "            sent_dict['Positive'] += 1\n",
    "    # Check Positive or Negative has higher values\n",
    "    if sent_dict['Positive'] > sent_dict['Negative']:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641287d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Siebert_Sentiment'] = df['Statement'].apply(lambda x: get_sentiment(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77227294",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Siebert_Sentiment'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a600a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c485ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = df['Siebert_Sentiment'].value_counts()\n",
    "\n",
    "# Create bar plot\n",
    "plt.figure(figsize=(6, 4))\n",
    "p = sns.barplot(x=value_counts.index, y=value_counts.values, color='#035397')\n",
    "show_values(p)\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Sentiment Analysis of Statements')\n",
    "plt.xticks(fontsize=8)\n",
    "plt.yticks(fontsize=8)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f46ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Length'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44817164",
   "metadata": {},
   "source": [
    "### Import databreaches Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9653cde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataframe with Description split into 2 columns: DescriptionClean (text only - need more cleaning) & WebsiteLink\n",
    "\n",
    "columns_to_read = ['Name', 'Day', 'Month', 'Year', 'NotificationDate', 'WebpageTitle', 'Author',\n",
    "                   'DetailedExplanation', 'Whitewashing', 'Apology', 'Compensation',\n",
    "                  'ResponsiveAction', 'ValueCommitment', 'CustomerFocus', 'OpenDisclosure', 'CustomerAdvice']\n",
    "df_st = pd.read_csv('ausdatabreach2018-23v1.csv', usecols=columns_to_read, encoding='latin1')\n",
    "df_st.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570879e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_st.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa7b798",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_st.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9943d6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Name'].equals(df_st['Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b49a206",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = df['Name'] != df_st['Name']\n",
    "\n",
    "# Get the indices of the differing rows\n",
    "differing_rows = comparison[comparison].index\n",
    "\n",
    "# Print the differing rows\n",
    "print(differing_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c792e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_st['Name'].iloc[39:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778fc092",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Name'].iloc[39:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b902c9",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d63a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63879274",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ed07e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d3958b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e214c784",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Calculate value counts\n",
    "value_counts = df['Year'].value_counts()\n",
    "\n",
    "# Create bar plot\n",
    "p = sns.barplot(x=value_counts.index, y=value_counts.values, color='#035397')\n",
    "show_values(p)\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Number of breaches by year')\n",
    "plt.xticks(fontsize=8)\n",
    "plt.yticks(fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0984fec5",
   "metadata": {},
   "source": [
    "### Percentages of each characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29011bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DetailedExplanation'].value_counts(normalize=True)[1]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04baedb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up the figure and axes\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "boolean_col = ['DetailedExplanation', 'Whitewashing', 'Apology', \n",
    "               'Compensation', 'ResponsiveAction', 'ValueCommitment',\n",
    "                'CustomerFocus', 'OpenDisclosure', 'CustomerAdvice']\n",
    "sns.countplot(data=df, x=\"DetailedExplanation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f7e468",
   "metadata": {},
   "outputs": [],
   "source": [
    "boolean_col = ['DetailedExplanation', 'Whitewashing', 'Apology', \n",
    "               'Compensation', 'ResponsiveAction', 'ValueCommitment',\n",
    "                'CustomerFocus', 'OpenDisclosure', 'CustomerAdvice']\n",
    "\n",
    "values = []\n",
    "for col in boolean_col:\n",
    "    pct = df[col].value_counts(normalize=True)[1]*100\n",
    "    values.append(pct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c23742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bar plot\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "p = sns.barplot(x=boolean_col, y=values, color='#035397')\n",
    "show_values(p)\n",
    "plt.xlabel('Characteristics')\n",
    "plt.ylabel('Percentages')\n",
    "plt.title('Percentages of Characteristics')\n",
    "plt.xticks(fontsize=7)\n",
    "plt.yticks(fontsize=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2b5e1c",
   "metadata": {},
   "source": [
    "### Response Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a63c282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FullTransparency\n",
    "def type1(row):\n",
    "    \n",
    "    if (row['DetailedExplanation'] == 1) & (row['Whitewashing'] == 0) & (row['ResponsiveAction'] == 1) & (row['ValueCommitment'] == 1)  & (row['OpenDisclosure'] == 1):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Apply the custom function to create a new column\n",
    "df['FullTransparency'] = df.apply(type1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7899fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['FullTransparency']==1].iloc[:,7:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b20b68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guarded\n",
    "def type2(row):\n",
    "    \n",
    "    if (row['DetailedExplanation'] == 0) & (row['Whitewashing'] == 1) & (row['ResponsiveAction'] == 1) & (row['ValueCommitment'] == 1)  & (row['OpenDisclosure'] == 0):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Apply the custom function to create a new column\n",
    "df['Guarded'] = df.apply(type2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf21472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opacity\n",
    "def type3(row):\n",
    "    \n",
    "    if (row['DetailedExplanation'] == 0) & (row['Whitewashing'] == 1) & (row['ResponsiveAction'] == 1) & (row['ValueCommitment'] == 0)  & (row['OpenDisclosure'] == 0):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Apply the custom function to create a new column\n",
    "df['Opacity'] = df.apply(type3, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d95c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CustomerInterest\n",
    "def type4(row):\n",
    "    \n",
    "    if (row['Apology'] == 1) & (row['Compensation'] == 1) & (row['CustomerFocus'] == 1) & (row['CustomerAdvice'] == 1):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Apply the custom function to create a new column\n",
    "df['CustomerInterest'] = df.apply(type4, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdcffe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CustomerInterest\n",
    "def type5(row):\n",
    "    \n",
    "    if (row['Apology'] == 1) & (row['Compensation'] == 0):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Apply the custom function to create a new column\n",
    "df['BalancedInterest'] = df.apply(type5, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb7e66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CustomerInterest\n",
    "def type6(row):\n",
    "    \n",
    "    if (row['Apology'] == 0) & (row['Compensation'] == 0) & (row['CustomerFocus'] == 0) & (row['CustomerAdvice'] == 0):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Apply the custom function to create a new column\n",
    "df['CompanyInterest'] = df.apply(type6, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614b0d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:5,7:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20003b6a",
   "metadata": {},
   "source": [
    "### Percentages of each response types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62073ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_col = ['FullTransparency', 'Guarded', 'Opacity', \n",
    "               'CustomerInterest', 'BalancedInterest', 'CompanyInterest']\n",
    "\n",
    "values_pct = []\n",
    "for col in response_col:\n",
    "    # If there is only one value \n",
    "    if len(df[col].value_counts(normalize=True)) == 1:\n",
    "        values_pct.append(0)\n",
    "        \n",
    "    else:\n",
    "        pct = df[col].value_counts(normalize=True)[1]*100\n",
    "        \n",
    "        values_pct.append(pct)\n",
    "    \n",
    "# Create bar plot\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "p = sns.barplot(x=response_col, y=values_pct, color='#035397')\n",
    "show_values(p)\n",
    "plt.xlabel('Response Types')\n",
    "plt.ylabel('Percentages')\n",
    "plt.title('Percentages of Response Types')\n",
    "plt.xticks(fontsize=5)\n",
    "plt.yticks(fontsize=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49810c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BalancedInterest'].value_counts(normalize=True)[1]*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b19a3db",
   "metadata": {},
   "source": [
    "### Bigrams for titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8ddbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245a9208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref: https://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n",
    "\n",
    "# Lemmatize with POS Tag\n",
    "from nltk.corpus import wordnet\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f875f27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    try: \n",
    "        # split into sentences\n",
    "        sentences = sent_tokenize(text)\n",
    "\n",
    "        # split into words\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        # convert to lower case\n",
    "        tokens = [w.lower() for w in tokens]\n",
    "\n",
    "        # remove punctuation from each word\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        stripped = [w.translate(table) for w in tokens]\n",
    "\n",
    "        # remove remaining tokens that are not alphabetic\n",
    "        words = [word for word in stripped if word.isalpha()]\n",
    "\n",
    "        # filter out stop words\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        words = [w for w in words if not w in stop_words]\n",
    "\n",
    "        # Lemmatise the tokens\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in words]\n",
    "\n",
    "        # Join the tokens back into a string\n",
    "        processed_text = ' '.join(lemmatized_tokens)\n",
    "        return processed_text\n",
    "    except TypeError:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e541eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Title_nltk'] = df['WebpageTitle'].apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03cf637",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Title_nltk'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7f26de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "def bigrams_convert(column, n=2):\n",
    "    df['bigrams'+'_'+column]=df[column].apply(lambda sentence: list(ngrams(sentence.split(), n)))\n",
    "    \n",
    "def trigrams_convert(column, n=3):\n",
    "    df['trigrams'+'_'+column]=df[column].apply(lambda sentence: list(ngrams(sentence.split(), n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0dd416",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_convert('Title_nltk')\n",
    "trigrams_convert('Title_nltk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde8b6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcb0942",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "bigrams = []\n",
    "for row in df['bigrams_Title_nltk']:\n",
    "    bigrams.extend(row)\n",
    "    \n",
    "print(f'Count of bigrams: {len(bigrams)}')\n",
    "print(\"\\n\")\n",
    "    \n",
    "# Count the frequency of each bigram\n",
    "bigram_frequency = Counter(bigrams)\n",
    "\n",
    "# Print the frequency of each bigram\n",
    "# for bigram, frequency in bigram_frequency.items():\n",
    "#     print(bigram, frequency)\n",
    "    \n",
    "# Organize elements by frequency using most_common()\n",
    "bigrams_organized_by_frequency = bigram_frequency.most_common()\n",
    "\n",
    "# Print the elements organized by frequency\n",
    "for element, frequency in bigrams_organized_by_frequency:\n",
    "    print(element, frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c48cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "bigrams_organized_by_frequency_10 = bigram_frequency.most_common(5)\n",
    "\n",
    "# Print the elements organized by frequency\n",
    "# print('Top 10 trigrams')\n",
    "bi = []\n",
    "freq = []\n",
    "\n",
    "for element, frequency in bigrams_organized_by_frequency_10:\n",
    "    bigram = ' '.join(element)\n",
    "    print(f'Bigram: {bigram} - Frequency: {frequency}')\n",
    "    bi.append(bigram)\n",
    "    freq.append(frequency)\n",
    "\n",
    "\n",
    "df_bi = pd.DataFrame({'Bigram': bi, 'Count': freq})\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "ax = sns.barplot(x=df_bi['Bigram'].values, y=df_bi['Count'].values)\n",
    "\n",
    "# Display the count on each bar\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_height(), '.0f'), (p.get_x() + p.get_width() / 2., p.get_height()), ha = 'center', va = 'center', xytext = (0, 5), textcoords = 'offset points')\n",
    "\n",
    "plt.ylabel('Count')\n",
    "plt.title('Top 5 Most Frequently Occuring Bigrams')\n",
    "\n",
    "plt.xticks(rotation=70)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4215844",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = []\n",
    "for row in df['trigrams_Title_nltk']:\n",
    "    trigrams.extend(row)\n",
    "    \n",
    "print(f'Count of trigrams: {len(trigrams)}')\n",
    "print(\"\\n\")\n",
    "    \n",
    "# Count the frequency of each trigram\n",
    "trigram_frequency = Counter(trigrams)\n",
    "\n",
    "# Organize elements by frequency using most_common()\n",
    "trigrams_organized_by_frequency = trigram_frequency.most_common()\n",
    "\n",
    "# Print the elements organized by frequency\n",
    "for element, frequency in trigrams_organized_by_frequency:\n",
    "    print(element, frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4fc838",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams_organized_by_frequency_10 = trigram_frequency.most_common(5)\n",
    "\n",
    "# Print the elements organized by frequency\n",
    "# print('Top 10 trigrams')\n",
    "tri = []\n",
    "freq = []\n",
    "\n",
    "for element, frequency in trigrams_organized_by_frequency_10:\n",
    "    trigram = ' '.join(element)\n",
    "#     print(f'Trigram: {trigram} - Frequency: {frequency}')\n",
    "    tri.append(trigram)\n",
    "    freq.append(frequency)\n",
    "    \n",
    "df_tri = pd.DataFrame({'Trigram': tri, 'Count': freq})\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "ax = sns.barplot(x=df_tri['Trigram'].values, y=df_tri['Count'].values)\n",
    "\n",
    "# Display the count on each bar\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_height(), '.0f'), (p.get_x() + p.get_width() / 2., p.get_height()), ha = 'center', va = 'center', xytext = (0, 5), textcoords = 'offset points')\n",
    "\n",
    "plt.ylabel('Count')\n",
    "plt.title('Top 5 Most Frequently Occuring Trigrams')\n",
    "\n",
    "plt.xticks(rotation=70)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9225eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate value counts\n",
    "value_counts = df['Author'].value_counts(dropna=False)\n",
    "\n",
    "# Create bar plot\n",
    "plt.figure(figsize=(9, 5))\n",
    "p = sns.barplot(x=value_counts.index, y=value_counts.values, color='#035397')\n",
    "show_values(p)\n",
    "plt.xlabel('Author')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Authors of Statements')\n",
    "plt.xticks(fontsize=8)\n",
    "plt.yticks(fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b3e4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Author'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c000d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_excel('output.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c53ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.drop(['CleanStatement2'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68961af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize NLTK sentiment analyzer\n",
    "# nltk.download('vader_lexicon')\n",
    "# from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "# analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# # create get_sentiment function\n",
    "# def get_sentiment(text):\n",
    "#     scores = analyzer.polarity_scores(text)\n",
    "#     sentiment = 1 if scores['pos'] > 0 else 0\n",
    "#     return sentiment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
